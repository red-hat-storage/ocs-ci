tests:
   - test:
       name: install ceph pre-requisites
       module: install_prereq.py
       abort-on-fail: true

   - test:
       name: node update to latest kernel
       module: update-node.py
       abort-on-fail: true
   - test:
      name: ceph ansible
      module: test_ansible.py
      config:
        ansi_config:
        ceph_test: True
        ceph_origin: distro
        ceph_stable_release: jewel
        ceph_repository: rhcs
        journal_size: 1024
        ceph_stable: True
        fetch_directory: ~/fetch
        copy_admin_key: true
        ceph_conf_overrides:
            global:
              osd_pool_default_pg_num: 64
              osd_default_pool_size: 2
              osd_pool_default_pgp_num: 64
              mon_max_pg_per_osd: 1024
            mon:
              mon_allow_pool_delete: true
              debug mon: 5
            mds:
              mds_bal_split_size: 100
              mds_bal_merge_size: 5
              mds_bal_fragment_size_max: 10000
              debug mds: 5
                      cephfs_pools:
                        - name: "cephfs_data"
                          pgs: "64"
                        - name: "cephfs_metadata"
                          pgs: "64"
      desc: test cluster setup using ceph-ansible
      destroy-cluster: False
      abort-on-fail: true

   - test:
       name: multi-client-rw-io
       module: CEPH-10528_10529.py
       polarion-id: CEPH-10528,CEPH-10529
       desc: Single CephFS on multiple clients,performing IOs and checking file locking mechanism
       abort-on-fail: false

   - test:
       name: cephfs-io
       module: CEPH-11220.py
       polarion-id: CEPH-11220
       desc: Mount CephFS on multiple clients, On same directory perform write from some clients and do read that data from some clients
       abort-on-fail: false
   - test:
       name: cephfs-stressIO
       module: CEPH-11222_11223.py
       polarion-id: CEPH-11222,CEPH-11223
       config:
         num_of_osds: 12
       desc: Mount CephFS on multiple clients and run Stress IO from all clients on different directories
       abort-on-fail: false
   - test:
       name: cephfs-rsync
       module: CEPH-11298.py
       polarion-id: CEPH-11298
       desc: Mount CephFS on multiple clients and rsync files and directories to outside the filesystem and vice-versa
       abort-on-fail: false
   - test:
       name: cephfs-filelayouts
       module: CEPH-11334.py
       polarion-id: CEPH-11334
       desc: Mount CephFS on multiple clients and perform file layout operations
       abort-on-fail: false
   - test:
       name: fs_client-permissions
       module: CEPH-11338.py
       polarion-id: CEPH-11338
       desc: Mount CephFS on multiple clients,give various permissions to client like only read,write etc
       abort-on-fail: false
   - test:
       name: cephfs-basics
       module: cephfs_basic_tests.py
       polarion-id: CEPH-11293,CEPH-11296,CEPH-11297,CEPH-11295
       desc: cephfs basic operations
       abort-on-fail: false

   - test:
       name: posix compliance tests
       module: pjd.py
       polarion-id: CEPH-11269,CEPH-11270,CEPH-11271,CEPH-11272,CEPH-11273,CEPH-11274,CEPH-11275,CEPH-11276,CEPH-11277,CEPH-11278,CEPH-11279,CEPH-11280, CEPH-11281, CEPH-11282, CEPH-11283,CEPH-11284,CEPH-11285,CEPH-11286,CEPH-11287,CEPH-11288,CEPH-11289,CEPH-11290,CEPH-11291,CEPH-11292,CEPH-11294,CEPH-11304,CEPH-11305,CEPH-11281,CEPH-11282,CEPH-11283
       desc: Mount CephFS on multiple clients and run pjd tests
       abort-on-fail: false
