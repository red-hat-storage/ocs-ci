tests:
   - test:
       name: install ceph pre-requisities
       module: install_prereq.py
       abort-on-fail: true
   - test:
      name: ceph ansible
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                mds:
                  mds_bal_split_size: 100
                  mds_bal_merge_size: 5
                  mds_bal_fragment_size_max: 10000
                  debug mds: 5
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: test cluster setup using ceph-ansible
      destroy-cluster: False
      abort-on-fail: true
   - test:
       name: multiple clients read/writing io
       module: CEPH-10528_10529.py
       polarion-id: CEPH-10528,CEPH-10529
       desc: Testing Single CephFS on multiple clients,performing IOs and checking file locking mechanism
       abort-on-fail: true
   - test:
       name: cephfs_mds_failover
       module: CEPH-11228.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11228
       desc: Testing MDSfailover on active-active mdss,performing client IOs with max:min directory pinning with 2 active mdss
       abort-on-fail: true
   - test:
       name: cephfs_mds_failover
       module: CEPH-11229.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11229
       desc: Testing MDSfailover on active-active mdss,performing client IOs with 10:2 directory pinning with 2 active mdss
       abort-on-fail: true
   - test:
       name: cephfs_mds_failover
       module: CEPH-11230.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11230
       desc: Testing MDSfailover on active-active mdss,performing client IOs with equal directory pinning with 2 active mdss
       abort-on-fail: true
