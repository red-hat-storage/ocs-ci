tests:
    - test:
        name: install ceph pre-requisites
        module: install_prereq.py
        abort-on-fail: true
 
    - test:
        name: containerized ceph ansible
        polarion-id: CEPH-83571503
        module: test_ansible.py
        config:
          ansi_config:
            ceph_test: True
            ceph_stable_release: jewel
            ceph_origin: distro
            ceph_repository: rhcs
            journal_collocation: True
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            ceph_docker_image: rhceph/rhceph-2-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
        desc: osd with collocated journal
        destroy-cluster: False
        abort-on-fail: true

#   - test:
#      name: librbd workunit
#      module: test_workunit.py
#      config:
#            test_name: rbd/test_librbd_python.sh
#            branch: jewel
#            role: mon
#      desc: Test librbd unit tests
#   - test:
#      name: rbd cli automation
#      module: rbd_system.py
#      config:
#            test_name: rbd_cli_automation.py
#            branch: jewel
#      desc: Test rbd cli automation tests
#   - test:
#      name: check-ceph-health
#      module: exec.py
#      config:
#            cmd: ceph -s
#            sudo: True
#      desc: Check for ceph health debug info
#   - test:
#      name: rados_bench_test
#      module: radosbench.py
#      config:
#            pg_num: '128'
#            pool_type: 'normal'
#      desc: run rados bench for 360 - normal profile

    - test:
        name: ceph ansible purge
        polarion-id: CEPH-83571493
        module: purge_cluster.py
        config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
        desc: Purge ceph cluster
 
    - test:
        name: containerized ceph ansible
        polarion-id: CEPH-83571502
        module: test_ansible.py
        config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: jewel
            ceph_repository: rhcs
            osd_scenario: collocated
            dmcrypt: True
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            ceph_docker_image: rhceph/rhceph-2-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
        desc: encrypted osd with collocated journal
        destroy-cluster: False
        abort-on-fail: true

#   - test:
#      name: librbd workunit
#      module: test_workunit.py
#      config:
#            test_name: rbd/test_librbd_python.sh
#            branch: luminous
#            role: mon
#      desc: Test librbd unit tests
#   - test:
#      name: rbd cli automation
#      module: rbd_system.py
#      config:
#            test_name: rbd_cli_automation.py
#            branch: master
#      desc: Test rbd cli automation tests
#   - test:
#      name: check-ceph-health
#      module: exec.py
#      config:
#            cmd: ceph -s
#            sudo: True
#      desc: Check for ceph health debug info
#   - test:
#      name: rados_bench_test
#      module: radosbench.py
#      config:
#            pg_num: '128'
#            pool_type: 'normal'
#      desc: run rados bench for 360 - normal profile

    - test:
        name: ceph ansible purge
        polarion-id: CEPH-83571493
        module: purge_cluster.py
        config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
        desc: Purge ceph cluster

    - test:
        name: containerized ceph ansible
        polarion-id: CEPH-83571493
        module: test_ansible.py
        config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: jewel
            ceph_repository: rhcs
            osd_scenario: non-collocated
            dedicated_devices:
              - /dev/vde
              - /dev/vde
              - /dev/vde
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            ceph_docker_image: rhceph/rhceph-2-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
        desc: osd with dedicated journal
        destroy-cluster: False
        abort-on-fail: true

#   - test:
#      name: librbd workunit
#      module: test_workunit.py
#      config:
#            test_name: rbd/test_librbd_python.sh
#            branch: luminous
#            role: mon
#      desc: Test librbd unit tests
#   - test:
#      name: rbd cli automation
#      module: rbd_system.py
#      config:
#            test_name: rbd_cli_automation.py
#            branch: master
#      desc: Test rbd cli automation tests
#   - test:
#      name: check-ceph-health
#      module: exec.py
#      config:
#            cmd: ceph -s
#            sudo: True
#      desc: Check for ceph health debug info
#   - test:
#      name: rados_bench_test
#      module: radosbench.py
#      config:
#            pg_num: '128'
#            pool_type: 'normal'
#      desc: run rados bench for 360 - normal profile

    - test:
        name: ceph ansible purge
        polarion-id: CEPH-83571493
        module: purge_cluster.py
        config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
        desc: Purge ceph cluster

    - test:
        name: containerized ceph ansible
        polarion-id: CEPH-83571495
        module: test_ansible.py
        config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: jewel
            ceph_repository: rhcs
            osd_scenario: non-collocated
            dmcrypt: True
            dedicated_devices:
              - /dev/vde
              - /dev/vde
              - /dev/vde
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            ceph_docker_image: rhceph/rhceph-2-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
        desc: encrypted osd with dedicated journal
        destroy-cluster: False
        abort-on-fail: true

#   - test:
#      name: librbd workunit
#      module: test_workunit.py
#      config:
#            test_name: rbd/test_librbd_python.sh
#            branch: jewel
#            role: mon
#      desc: Test librbd unit tests
#   - test:
#      name: rbd cli automation
#      module: rbd_system.py
#      config:
#            test_name: rbd_cli_automation.py
#            branch: jewel
#      desc: Test rbd cli automation tests
#   - test:
#      name: check-ceph-health
#      module: exec.py
#      config:
#            cmd: ceph -s
#            sudo: True
#      desc: Check for ceph health debug info
#   - test:
#      name: rados_bench_test
#      module: radosbench.py
#      config:
#            pg_num: '128'
#            pool_type: 'normal'
#      desc: run rados bench for 360 - normal profile

    - test:
        name: ceph ansible purge
        polarion-id: CEPH-83571493
        module: purge_cluster.py
        config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
        desc: Purge ceph cluster

    - test:
        name: containerized ceph ansible
        polarion-id: CEPH-83571501
        module: test_ansible.py
        config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: jewel
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            ceph_rhcs_iso_install: true
            ceph_rhcs_iso_path: ~/ceph-ansible/iso/ceph.iso
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            ceph_docker_image: rhceph/rhceph-2-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
        desc: iso source (osd with collocated journal)
        destroy-cluster: False
        abort-on-fail: true

#   - test:
#      name: librbd workunit
#      module: test_workunit.py
#      config:
#            test_name: rbd/test_librbd_python.sh
#            branch: jewel
#            role: mon
#      desc: Test librbd unit tests
#   - test:
#      name: rbd cli automation
#      module: rbd_system.py
#      config:
#            test_name: rbd_cli_automation.py
#            branch: jewel
#      desc: Test rbd cli automation tests
#   - test:
#      name: check-ceph-health
#      module: exec.py
#      config:
#            cmd: ceph -s
#            sudo: True
#      desc: Check for ceph health debug info
#   - test:
#      name: rados_bench_test
#      module: radosbench.py
#      config:
#            pg_num: '128'
#            pool_type: 'normal'
#      desc: run rados bench for 360 - normal profile

    - test:
        name: ceph ansible purge
        polarion-id: CEPH-83571493
        module: purge_cluster.py
        config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
        desc: Purge ceph cluster

    - test:
        name: containerized ceph ansible
        polarion-id: CEPH-83571499
        module: test_ansible.py
        config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: jewel
            ceph_repository: rhcs
            osd_scenario: collocated
            dmcrypt: True
            osd_auto_discovery: True
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            ceph_docker_image: rhceph/rhceph-2-rhel7
            ceph_docker_image_tag: latest
            containerized_deployment: true
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
        desc: osd with collocated journal and autodiscovery
        destroy-cluster: False
        abort-on-fail: true

#   - test:
#      name: librbd workunit
#      module: test_workunit.py
#      config:
#            test_name: rbd/test_librbd_python.sh
#            branch: jewel
#            role: mon
#      desc: Test librbd unit tests
#   - test:
#      name: rbd cli automation
#      module: rbd_system.py
#      config:
#            test_name: rbd_cli_automation.py
#            branch: jewel
#      desc: Test rbd cli automation tests
#   - test:
#      name: check-ceph-health
#      module: exec.py
#      config:
#            cmd: ceph -s
#            sudo: True
#      desc: Check for ceph health debug info
#   - test:
#      name: rados_bench_test
#      module: radosbench.py
#      config:
#            pg_num: '128'
#            pool_type: 'normal'
#      desc: run rados bench for 360 - normal profile

    - test:
        name: ceph ansible purge
        polarion-id: CEPH-83571493
        module: purge_cluster.py
        config:
            ansible-dir: /usr/share/ceph-ansible
            playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
        desc: Purge ceph cluster
        destroy-cluster: True
