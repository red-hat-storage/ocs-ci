tests:
   - test:
       name: install ceph pre-requisites
       module: install_prereq.py
       abort-on-fail: true
   - test:
      name: ceph ansible
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                  debug mon: 5
                mds:
                  mds_bal_split_size: 100
                  mds_bal_merge_size: 5
                  mds_bal_fragment_size_max: 10000
                  debug mds: 5
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: test cluster setup using ceph-ansible
      destroy-cluster: False
      abort-on-fail: true

   - test:
       name: recreate cephfs with ec pools
       module: recreate-cephfs-ecpool.py
       abort-on-fail: false

   - test:
       name: cephfs-lifecycle ops
       module: CEPH-11333.py
       polarion-id: CEPH-11333
       desc: Perfrom cephfs lifecycle ops like delete cephfs,recreate cephfs
       abort-on-fail: false

   - test:
       name: multi-client-rw-io
       module: CEPH-10528_10529.py
       polarion-id: CEPH-10528,CEPH-10529
       desc: Single CephFS on multiple clients,performing IOs and checking file locking mechanism
       abort-on-fail: false
   - test:
       name: cephfs-mds-failover
       module: CEPH-11228.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11228,CEPH-11239
       desc: MDSfailover on active-active mdss,performing client IOs with max:min directory pinning with 2 active mdss
       abort-on-fail: false
   - test:
       name: cephfs-mds-failover
       module: CEPH-11229.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11229
       desc: MDSfailover on active-active mdss,performing client IOs with 10:2 directory pinning with 2 active mdss
       abort-on-fail: false
   - test:
       name: cephfs-mds-failover
       module: CEPH-11230.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11230
       desc: MDSfailover on active-active mdss,performing client IOs with equal directory pinning with 2 active mdss
       abort-on-fail: false
   - test:
       name: cephfs-split-merge
       module: CEPH-11232_11233.py
       polarion-id: CEPH-11232,CEPH-11233
       desc: Performing stress IOs to check split and merge functions of mds
       abort-on-fail: false
   - test:
       name: mdsfailover-fs-utilites
       module: CEPH-11242.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11242
       desc: Testing fs utilites along with mds failover on active-active mdss
       abort-on-fail: false
   - test:
       name: read-write-del-ops
       module: CEPH-11219.py
       polarion-id: CEPH-11219,CEPH-11226,CEPH-11224
       config:
             num_of_osds: 12
       desc: Mount CephFS on multiple clients perform read and write operation on same file from different clients and perform delete ops
       abort-on-fail: false

   - test:
       name: cephfs-mdsfailover-pinning-io
       module: CEPH-11227.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11227
       desc: MDSfailover on active-active mdss,performing client IOs with no pinning at the first,later pin 10 dirs with IOs
       abort-on-fail: false
   - test:
       name: cephfs-io
       module: CEPH-11220.py
       polarion-id: CEPH-11220
       desc: Mount CephFS on multiple clients, On same directory perform write from some clients and do read that data from some clients
       abort-on-fail: false
   - test:
       name: cephfs-stressIO
       module: CEPH-11222_11223.py
       polarion-id: CEPH-11222,CEPH-11223
       config:
             num_of_osds: 12
       desc: Mount CephFS on multiple clients,
       abort-on-fail: false
   - test:
       name: MON-failure
       module: CEPH-11254_fuse.py
       polarion-id: CEPH-11254
       config:
         num_of_osds: 12
       desc: Mount CephFS on multiple clients with fstab entry ,perform MON reboot,service kill,network disconnections,client reboots, with IOs
       abort-on-fail: false
   - test:
       name: OSD-failure
       module: CEPH-11255_11336_fuse.py
       polarion-id: CEPH-11255,CEPH-11336
       config:
         num_of_osds: 12
       desc: Mount CephFS on multiple clients with fstab entry ,perform OSD reboot,service kill,network disconnections,client reboots, with IOs
       abort-on-fail: false
   - test:
       name: MDS-failure
       module: CEPH-11256_fuse.py
       polarion-id: CEPH-11256
       config:
         num_of_osds: 12
       desc: Mount CephFS on multiple clients,perform MDS reboot,service kill,network disconnections,client reboots, with IOs
       abort-on-fail: false
   - test:
       name: MDS-pinning-system_tests
       module: CEPH-11231.py
       polarion-id: CEPH-11231
       config:
         num_of_osds: 12
       desc: Active-active multi MDS with standby and pinning subtrees to all MDS, perform system tests like, OSD/MON/MDS reboot,service,restart
       abort-on-fail: false
   - test:
       name: cephfs-basics-ios
       module: CEPH-10625_11225.py
       polarion-id: CEPH-10625,CEPH-11225
       desc: Mount CephFS on multiple clients,running IOs,creating files and dirs with special chars
       abort-on-fail: false
   - test:
       name: cephfs-rsync
       module: CEPH-11298.py
       polarion-id: CEPH-11298
       desc: Mount CephFS on multiple clients and rsync files and directories to outside the filesystem and vice-versa
       abort-on-fail: false
   - test:
       name: cephfs-filelayouts
       module: CEPH-11334.py
       polarion-id: CEPH-11334
       desc: Mount CephFS on multiple clients and perform file layout operations
       abort-on-fail: false
   - test:
       name: cephfs_client-permissions
       module: CEPH-11338.py
       polarion-id: CEPH-11338
       desc: Mount CephFS on multiple clients,give various permissions to client like only read,write etc
       abort-on-fail: false
   - test:
       name: cephfs-basics
       module: cephfs_basic_tests.py
       polarion-id: CEPH-11293,CEPH-11296,CEPH-11297,CEPH-11295
       desc: cephfs basic operations
       abort-on-fail: false
   - test:
       name: nfs-ganesha_with_cephfs
       module: nfs-ganesha_basics.py
       desc: Mount cephfs on clients,configure nfs-ganesha on nfs server,do mount on any client and do IOs
       abort-on-fail: false
   - test:
       name: client-eviction
       module: CEPH-11335.py
       polarion-id: CEPH-11335
       desc: Mount CephFS on multiple clients and perform client eviction
       abort-on-fail: false
   - test:
       name: cephfs-osd-power-failre
       module: CEPH-11262.py
       polarion-id: CEPH-11262
       desc: Mount CephFS on multiple clients and do multiple client IOs, parallely do osd power failre
       abort-on-fail: false
