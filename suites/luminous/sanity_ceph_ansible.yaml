tests:
   - test:
      name: install ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571467
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with collocated journal
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: rbd cli image
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_image.py
            branch: master
      polarion-id: CEPH-83572722
      desc: CLI validation for image related commands

   - test:
      name: rbd cli snap_clone
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_snap_clone.py
            branch: master
      polarion-id: CEPH-83572725
      desc: CLI validation for snap and clone related commands

   - test:
      name: rbd cli misc
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_misc.py
            branch: master
      polarion-id: CEPH-83572724
      desc: CLI validation for miscellaneous rbd commands

   - test:
      name: check-ceph-health
      module: exec.py
      config:
            cmd: ceph -s
            sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
            pg_num: '128'
            pool_type: 'normal'
      desc: run rados bench for 360 - normal profile

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster
#        recreate-cluster: True
#
#   - test:
#       name: install ceph pre-requisites
#       module: install_prereq.py
#       abort-on-fail: true

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571492
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            dmcrypt: True
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: encrypted osd with collocated journal
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: rbd cli image
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_image.py
            branch: master
      polarion-id: CEPH-83572722
      desc: CLI validation for image related commands

   - test:
      name: rbd cli snap_clone
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_snap_clone.py
            branch: master
      polarion-id: CEPH-83572725
      desc: CLI validation for snap and clone related commands

   - test:
      name: rbd cli misc
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_misc.py
            branch: master
      polarion-id: CEPH-83572724
      desc: CLI validation for miscellaneous rbd commands

   - test:
      name: check-ceph-health
      module: exec.py
      config:
            cmd: ceph -s
            sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
            pg_num: '128'
            pool_type: 'normal'
      desc: run rados bench for 360 - normal profile

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster
#        recreate-cluster: True
#
#   - test:
#       name: install ceph pre-requisites
#       module: install_prereq.py
#       abort-on-fail: true

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571465
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: non-collocated
            dedicated_devices:
              - /dev/vde
              - /dev/vde
              - /dev/vde
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with dedicated journal
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: rbd cli image
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_image.py
            branch: master
      polarion-id: CEPH-83572722
      desc: CLI validation for image related commands

   - test:
      name: rbd cli snap_clone
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_snap_clone.py
            branch: master
      polarion-id: CEPH-83572725
      desc: CLI validation for snap and clone related commands

   - test:
      name: rbd cli misc
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_misc.py
            branch: master
      polarion-id: CEPH-83572724
      desc: CLI validation for miscellaneous rbd commands

   - test:
      name: check-ceph-health
      module: exec.py
      config:
            cmd: ceph -s
            sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
            pg_num: '128'
            pool_type: 'normal'
      desc: run rados bench for 360 - normal profile

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster
#        recreate-cluster: True
#
#   - test:
#       name: install ceph pre-requisites
#       module: install_prereq.py
#       abort-on-fail: true

   - test:
        name: ceph ansible
        polarion-id: CEPH-83571468
        module: test_ansible.py
        config:
          ansi_config:
              ceph_test: True
              ceph_origin: distro
              ceph_stable_release: luminous
              ceph_repository: rhcs
              osd_scenario: non-collocated
              dmcrypt: True
              dedicated_devices:
                - /dev/vde
                - /dev/vde
                - /dev/vde
              osd_auto_discovery: False
              journal_size: 1024
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
        desc: encrypted osd with dedicated journal
        destroy-cluster: False
        abort-on-fail: true

   - test:
      name: rbd cli image
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_image.py
            branch: master
      polarion-id: CEPH-83572722
      desc: CLI validation for image related commands

   - test:
      name: rbd cli snap_clone
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_snap_clone.py
            branch: master
      polarion-id: CEPH-83572725
      desc: CLI validation for snap and clone related commands

   - test:
      name: rbd cli misc
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_misc.py
            branch: master
      polarion-id: CEPH-83572724
      desc: CLI validation for miscellaneous rbd commands

   - test:
      name: check-ceph-health
      module: exec.py
      config:
            cmd: ceph -s
            sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
            pg_num: '128'
            pool_type: 'normal'
      desc: run rados bench for 360 - normal profile

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster
#        recreate-cluster: True
#
#   - test:
#       name: install ceph pre-requisites
#       module: install_prereq.py
#       abort-on-fail: true

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571500
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            ceph_rhcs_iso_install: true
            ceph_rhcs_iso_path: ~/ceph-ansible/iso/ceph.iso
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: iso source (osd with collocated journal)
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: rbd cli image
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_image.py
            branch: master
      polarion-id: CEPH-83572722
      desc: CLI validation for image related commands

   - test:
      name: rbd cli snap_clone
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_snap_clone.py
            branch: master
      polarion-id: CEPH-83572725
      desc: CLI validation for snap and clone related commands

   - test:
      name: rbd cli misc
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_misc.py
            branch: master
      polarion-id: CEPH-83572724
      desc: CLI validation for miscellaneous rbd commands

   - test:
      name: check-ceph-health
      module: exec.py
      config:
            cmd: ceph -s
            sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
            pg_num: '128'
            pool_type: 'normal'
      desc: run rados bench for 360 - normal profile

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster
#        recreate-cluster: True
#
#   - test:
#       name: install ceph pre-requisites
#       module: install_prereq.py
#       abort-on-fail: true

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571494
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: True
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with collocated journal and autodiscovery
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: rbd cli image
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_image.py
            branch: master
      polarion-id: CEPH-83572722
      desc: CLI validation for image related commands

   - test:
      name: rbd cli snap_clone
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_snap_clone.py
            branch: master
      polarion-id: CEPH-83572725
      desc: CLI validation for snap and clone related commands

   - test:
      name: rbd cli misc
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_misc.py
            branch: master
      polarion-id: CEPH-83572724
      desc: CLI validation for miscellaneous rbd commands

   - test:
      name: check-ceph-health
      module: exec.py
      config:
            cmd: ceph -s
            sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
            pg_num: '128'
            pool_type: 'normal'
      desc: run rados bench for 360 - normal profile

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571496
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            dmcrypt: True
            osd_auto_discovery: True
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: encrypted osd with collocated journal and autodiscovery
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: rbd cli image
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_image.py
            branch: master
      polarion-id: CEPH-83572722
      desc: CLI validation for image related commands

   - test:
      name: rbd cli snap_clone
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_snap_clone.py
            branch: master
      polarion-id: CEPH-83572725
      desc: CLI validation for snap and clone related commands

   - test:
      name: rbd cli misc
      module: rbd_system.py
      config:
            test_name: cli/rbd_cli_misc.py
            branch: master
      polarion-id: CEPH-83572724
      desc: CLI validation for miscellaneous rbd commands

   - test:
      name: check-ceph-health
      module: exec.py
      config:
            cmd: ceph -s
            sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
            pg_num: '128'
            pool_type: 'normal'
      desc: run rados bench for 360 - normal profile

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster
      destroy-cluster: False

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571467
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with collocated journal
      destroy-cluster: False
      abort-on-fail: False

   - test:
      name: config roll over
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      config:
          ansi_config:
              ceph_test: True
              ceph_origin: distro
              ceph_stable_release: luminous
              ceph_repository: rhcs
              osd_scenario: collocated
              osd_auto_discovery: False
              journal_size: 1024
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node15.*
                  demon:
                      - mon
      desc: add mon

   - test:
      name: config roll over
      polarion-id: CEPH-9583
      module: test_ansible_roll_over.py
      config:
          ansi_config:
              ceph_test: True
              ceph_origin: distro
              ceph_stable_release: luminous
              ceph_repository: rhcs
              osd_scenario: collocated
              osd_auto_discovery: False
              journal_size: 1024
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node16.*
                  demon:
                      - osd
      desc: add new osd node


   - test:
      name: config roll over
      polarion-id: CEPH-9582
      module: test_ansible_roll_over.py
      config:
          ansi_config:
              ceph_test: True
              ceph_origin: distro
              ceph_stable_release: luminous
              ceph_repository: rhcs
              osd_scenario: collocated
              osd_auto_discovery: False
              journal_size: 1024
              ceph_stable: True
              ceph_stable_rh_storage: True
              fetch_directory: ~/fetch
              copy_admin_key: true
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node16.*
                  demon:
                      - osd
      desc: add osd to existing node


   - test:
      name: shrink mon
      polarion-id: CEPH-9584
      module: shrink_mon.py
      config:
           mon-to-kill:
            - .*node8.*
      desc: remove monitor


   - test:
      name: shrink osd
      polarion-id: CEPH-9585
      module: shrink_osd.py
      config:
          osd-to-kill:
            - 2
      desc: shrink osd

   - test:
      name: ceph ansible purge
      polarion-id: CEPH-83571498
      module: purge_cluster.py
      config:
            ansible-dir: /usr/share/ceph-ansible
      desc: Purge ceph cluster
      destroy-cluster: True
