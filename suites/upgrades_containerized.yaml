tests:
   - test:
       name: install ceph pre-requisities
       module: install_prereq.py
       abort-on-fail: True
   - test:
      name: containerized ceph ansible
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            ceph_docker_image: rhceph/rhceph-3-rhel7
            containerized_deployment: True
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: True
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: ceph containerized installation
      destroy-cluster: False
      abort-on-fail: True
      desc: ceph containerized installation
      destroy-cluster: False
      abort-on-fail: True
   - test:
      name: containerized ceph ansible upgrade to rhcs 3.0z
      module: test_ansible_upgrade.py
      polarion-id: CEPH-83572731
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            containerized_deployment: True
            docker-insecure-registry: True
            ceph_docker_registry: brew-pulp-docker01.web.prod.ext.phx2.redhat.com:8888
            ceph_docker_image: rhceph
            ceph_docker_image_tag: ceph-3.0-rhel-7-containers-candidate-42352-20180613042051
            insecure_registry: True
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: True
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: ceph containerized rolling upgrade
      destroy-cluster: False
      abort-on-fail: True
   - test:
      name: librbd workunit
      module: test_workunit.py
      config:
         test_name: rbd/test_librbd_python.sh
         branch: luminous
         role: mon
      desc: Test librbd unit tests
   - test:
      name: rbd cli automation
      module: rbd_system.py
      config:
         test_name: rbd_cli_automation.py
         branch: master
      desc: Test rbd cli automation tests
   - test:
      name: check-ceph-health
      module: exec.py
      config:
         cmd: ceph -s
         sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
         pg_num: '128'
         pool_type: 'normal'
      desc: run rados bench for 360 - normal profile
   - test:
      name: s3 tests
      module: test_s3.py
      config:
         branch: ceph-luminous
      desc: execution of s3 tests against radosgw
      destroy-cluster: False
   - test:
      name: ceph ansible purge
      module: purge_cluster.py
      desc: Purge ceph cluster
      recreate-cluster: True

   - test:
       name: install ceph pre-requisities
       module: install_prereq.py
       abort-on-fail: True
   - test:
      name: containerized ceph ansible
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            ceph_docker_image: rhceph/rhceph-3-rhel7
            containerized_deployment: True
            ceph_docker_registry: registry.access.redhat.com
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: True
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: ceph containerized installation
      destroy-cluster: False
      abort-on-fail: True
      desc: ceph containerized installation
      destroy-cluster: False
      abort-on-fail: True
   - test:
      name: containerized ceph ansible upgrade to rhcs 3.x nightly
      module: test_ansible_upgrade.py
      polarion-id: CEPH-83572730
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            containerized_deployment: True
            docker-insecure-registry: True
            ceph_docker_registry: brew-pulp-docker01.web.prod.ext.phx2.redhat.com:8888
            ceph_docker_image: rhceph
            ceph_docker_image_tag: ceph-3.1-rhel-7-containers-candidate-49862-20180613163343
            insecure_registry: True
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: True
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: ceph containerized rolling upgrade
      destroy-cluster: False
      abort-on-fail: True
   - test:
      name: librbd workunit
      module: test_workunit.py
      config:
         test_name: rbd/test_librbd_python.sh
         branch: luminous
         role: mon
      desc: Test librbd unit tests
   - test:
      name: rbd cli automation
      module: rbd_system.py
      config:
         test_name: rbd_cli_automation.py
         branch: master
      desc: Test rbd cli automation tests
   - test:
      name: check-ceph-health
      module: exec.py
      config:
         cmd: ceph -s
         sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
         pg_num: '128'
         pool_type: 'normal'
      desc: run rados bench for 360 - normal profile
   - test:
      name: s3 tests
      module: test_s3.py
      config:
         branch: ceph-luminous
      desc: execution of s3 tests against radosgw
      destroy-cluster: False
   - test:
      name: ceph ansible purge
      module: purge_cluster.py
      desc: Purge ceph cluster
      destroy-cluster: False