import pytest
import logging
import time
import ocpnetsplit

from ocs_ci.framework.pytest_customization.marks import turquoise_squad
from ocs_ci.utility.retry import retry
from ocs_ci.ocs.exceptions import CommandFailed, CephHealthException


from ocs_ci.ocs.resources.stretchcluster import StretchCluster
from ocs_ci.ocs.exceptions import CephHealthException

from ocs_ci.ocs import constants
from ocs_ci.ocs.ocp import OCP
from ocs_ci.ocs.node import get_all_nodes, wait_for_nodes_status
from ocs_ci.helpers.sanity_helpers import Sanity
from datetime import datetime, timedelta, timezone
from ocs_ci.ocs.resources.pvc import get_pvc_objs
from ocs_ci.ocs.resources.pod import (
    Pod,
    wait_for_pods_to_be_in_statuses,
    get_pods_having_label,
    get_pod_node,
    get_ceph_tools_pod,
    get_mon_pod_id,
    get_mon_pods,
)
from ocs_ci.helpers.stretchcluster_helpers import (
    fetch_connection_scores_for_mon,
    get_mon_quorum_ranks,
)

logger = logging.getLogger(__name__)


@retry(CommandFailed, tries=4, delay=5)
def get_logfile_map_from_logwriter_pods(logwriter_pods, is_rbd=False):
    """
    This function fetches all the logfiles generated by logwriter instances
    and maps it with a string representing the start time of the logging

    Args:
        logwriter_pods (List): List of logwriter pod objects
        is_rbd (bool): True if it's an RBD RWO workload else False
    Returns:
        Dict: Representing map containing file name key and start time value

    """
    log_file_map = {}

    if not is_rbd:
        for file_name in list(
            filter(
                None,
                (
                    logwriter_pods[0]
                    .exec_sh_cmd_on_pod(command="ls -l | awk 'NR>1' | awk '{print $9}'")
                    .split("\n")
                ),
            )
        ):
            start_time = (
                logwriter_pods[0]
                .exec_sh_cmd_on_pod(command=f"cat {file_name} | grep -i started")
                .split(" ")[0]
            )
            log_file_map[file_name] = start_time.split("T")[1]
    else:
        for logwriter_pod in logwriter_pods:
            log_file_map[logwriter_pod.name] = {}
            for file_name in logwriter_pod.exec_sh_cmd_on_pod(
                command="ls -l | awk 'NR>1' | awk '{print $9}'"
            ).split("\n"):
                if file_name not in ("", "lost+found"):
                    start_time = logwriter_pod.exec_sh_cmd_on_pod(
                        command=f"cat {file_name} | grep -i started"
                    ).split(" ")[0]
                    log_file_map[logwriter_pod.name][file_name] = start_time.split("T")[
                        1
                    ]

    return log_file_map


@turquoise_squad
class TestNetSplit:
    @pytest.fixture()
    def init_sanity(self, request):
        """
        Initial Cluster sanity
        """
        self.sanity_helpers = Sanity()

        def finalizer():
            """
            Make sure the ceph health is OK at the end of the test
            """
            try:
                logger.info("Making sure ceph health is OK")
                self.sanity_helpers.health_check(tries=50)
            except CephHealthException as e:
                assert all(
                    err in e.args[0]
                    for err in ["HEALTH_WARN", "daemons have recently crashed"]
                ), f"[CephHealthException]: {e.args[0]}"
                get_ceph_tools_pod().exec_ceph_cmd(ceph_cmd="ceph crash archive-all")
                logger.info("Archived ceph crash!")

        request.addfinalizer(finalizer)

    @pytest.mark.parametrize(
        argnames="zones, duration",
        argvalues=[
            pytest.param(constants.NETSPLIT_DATA_1_DATA_2, 15),
            pytest.param(constants.NETSPLIT_ARBITER_DATA_1, 15),
            pytest.param(constants.NETSPLIT_ARBITER_DATA_1_AND_ARBITER_DATA_2, 15),
            pytest.param(constants.NETSPLIT_ARBITER_DATA_1_AND_DATA_1_DATA_2, 15),
        ],
        ids=[
            "Data-1-Data-2",
            "Arbiter-Data-1",
            "Arbiter-Data-1-and-Arbiter-Data-2",
            "Arbiter-Data-1-and-Data-1-Data-2",
        ],
    )
    def test_netsplit_cephfs(
        self,
        setup_logwriter_cephfs_workload_factory,
        setup_logwriter_rbd_workload_factory,
        logreader_workload_factory,
        nodes,
        zones,
        duration,
        init_sanity,
        reset_conn_score,
    ):
        """
        This test will test the netsplit scenarios when active-active CephFS workload
        is running.
        Steps:
            1) Run both the logwriter and logreader CephFS workload using single RWX volume
            2) Induce the network split
            3) Make sure logreader job pods have Completed state.
               Check if there is any write or read pause. Fail only when neccessary.
            4) For bc/ab-bc netsplit cases, it is expected for logreader/logwriter pods to go CLBO
               Make sure the above pods run fine after the nodes are restarted
            5) Delete the old logreader job and create new logreader job to verify the data corruption
            6) Make sure there is no data loss
            7) Do a complete cluster sanity and make sure there is no issue post recovery

        """

        sc_obj = StretchCluster()

        # run cephfs workload for both logwriter and logreader
        (
            sc_obj.cephfs_logwriter_dep,
            sc_obj.cephfs_logreader_job,
        ) = setup_logwriter_cephfs_workload_factory(read_duration=(duration + 5))
        logger.info("Workloads are running")

        # Generate 5 minutes worth of logs before inducing the netsplit
        logger.info("Generating 5 mins worth of log")
        time.sleep(300)

        # note all the pod names
        sc_obj.get_logwriter_reader_pods(label=constants.LOGWRITER_CEPHFS_LABEL)
        sc_obj.get_logwriter_reader_pods(label=constants.LOGREADER_CEPHFS_LABEL)
        sc_obj.get_logwriter_reader_pods(
            label=constants.LOGWRITER_RBD_LABEL, exp_num_replicas=2
        )

        # note the file names created and each file start write time
        # note the file names created
        sc_obj.get_logfile_map(label=constants.LOGWRITER_CEPHFS_LABEL)
        sc_obj.get_logfile_map(label=constants.LOGWRITER_RBD_LABEL)

        # note the start time (UTC)
        target_time = datetime.now() + timedelta(minutes=5)
        start_time = target_time.astimezone(timezone.utc)
        ocpnetsplit.main.schedule_split(
            nodes=get_all_nodes(),
            split_name=zones,
            target_dt=target_time,
            target_length=duration,
        )
        logger.info(f"Netsplit induced at {start_time} for zones {zones}")

        # note the end time (UTC)
        assert sc_obj.check_ceph_accessibility(
            timeout=((duration + 5) * 60)
        ), "Something went wrong. not expected. please check rook-ceph logs"
        end_time = datetime.now(timezone.utc)
        logger.info(f"Ended netsplit at {end_time}")

        # wait for the logreader workload to finish
        sc_obj.get_logwriter_reader_pods(label=constants.LOGWRITER_CEPHFS_LABEL)
        sc_obj.get_logwriter_reader_pods(label=constants.LOGREADER_CEPHFS_LABEL)
        sc_obj.get_logwriter_reader_pods(
            label=constants.LOGWRITER_RBD_LABEL, exp_num_replicas=2
        )

        # check if all the read operations are successful during the failure window, check for every minute
        sc_obj.post_failure_checks(
            start_time,
            end_time,
        )

        # reboot the nodes where the pods are not running
        # TODO: Improve the below piece of code
        pods_not_running = [
            pod
            for pod in sc_obj.cephfs_logwriter_pods
            if OCP(
                kind="Pod", namespace=constants.STRETCH_CLUSTER_NAMESPACE
            ).get_resource_status(pod.name)
            != constants.STATUS_RUNNING
        ]
        assert (
            zones in ("bc", "ab-bc") or len(pods_not_running) == 0
        ), f"Unexpectedly these pods {pods_not_running} are not running after the {zones} failure"

        for pod in pods_not_running:
            node_obj = get_pod_node(pod)
            nodes.stop_nodes(nodes=[node_obj], wait=False)
            wait_for_nodes_status(
                node_names=[node_obj.name], status=constants.NODE_NOT_READY
            )
            nodes.start_nodes(nodes=[node_obj])
            wait_for_nodes_status(node_names=[node_obj.name])

        wait_for_pods_to_be_in_statuses(
            expected_statuses=constants.STATUS_RUNNING,
            pod_names=[pod.name for pod in sc_obj.cephfs_logwriter_pods],
            timeout=900,
            namespace=constants.STRETCH_CLUSTER_NAMESPACE,
        )

        sc_obj.cephfs_logreader_job.delete()
        logger.info(sc_obj.cephfs_logreader_pods)
        for pod in sc_obj.cephfs_logreader_pods:
            pod.wait_for_pod_delete(timeout=120)
        logger.info("All old CephFS logreader pods are deleted")

        # check for any data loss
        assert sc_obj.check_for_data_loss(
            constants.LOGWRITER_CEPHFS_LABEL
        ), "[CephFS] Data is lost"
        logger.info("[CephFS] No data loss is seen")
        assert sc_obj.check_for_data_loss(
            constants.LOGWRITER_RBD_LABEL
        ), "[RBD] Data is lost"
        logger.info("[RBD] No data loss is seen")

        # check for data corruption
        pvc = get_pvc_objs(
            pvc_names=[
                sc_obj.cephfs_logwriter_dep.get()["spec"]["template"]["spec"][
                    "volumes"
                ][0]["persistentVolumeClaim"]["claimName"]
            ],
            namespace=constants.STRETCH_CLUSTER_NAMESPACE,
        )[0]
        logreader_workload_factory(
            pvc=pvc, logreader_path=constants.LOGWRITER_CEPHFS_READER, duration=5
        )
        logger.info("Getting new logreader pods!")
        new_logreader_pods = [
            Pod(**pod).name
            for pod in get_pods_having_label(
                label="app=logreader-cephfs",
                namespace=constants.STRETCH_CLUSTER_NAMESPACE,
                statuses=["Running", "Completed"],
            )
        ]
        for pod in sc_obj.cephfs_logreader_pods:
            if pod.name in new_logreader_pods:
                new_logreader_pods.remove(pod.name)

        sc_obj.cephfs_logreader_pods = new_logreader_pods
        logger.info(f"New logreader pods: {new_logreader_pods}")

        wait_for_pods_to_be_in_statuses(
            expected_statuses=constants.STATUS_COMPLETED,
            pod_names=[pod_name for pod_name in new_logreader_pods],
            timeout=900,
            namespace=constants.STRETCH_CLUSTER_NAMESPACE,
        )
        logger.info("Logreader job pods have reached 'Completed' state!")

        assert sc_obj.check_for_data_corruption(
            label=constants.LOGREADER_CEPHFS_LABEL
        ), "Data is corrupted for cephFS workloads"
        logger.info("No data corruption is seen in CephFS workloads")

        assert sc_obj.check_for_data_corruption(
            label=constants.LOGWRITER_RBD_LABEL
        ), "Data is corrupted for RBD workloads"
        logger.info("No data corruption is seen in RBD workloads")

        # check the connection scores if its clean
        mon_conn_score_map = {}
        mon_pods = get_mon_pods()
        for pod in mon_pods:
            mon_conn_score_map[get_mon_pod_id(pod)] = fetch_connection_scores_for_mon(
                pod
            )
        logger.info("Fetched connection scores for all the mons!!")
        mon_quorum_ranks = get_mon_quorum_ranks()
        logger.info(f"Current mon_quorum ranks : {mon_quorum_ranks}")

        # check the connection score if it's clean
        sc_obj.validate_conn_score(mon_conn_score_map, mon_quorum_ranks)
