import logging
import json
import re
import time

from ocs_ci.utility.retry import retry
from ocs_ci.ocs.exceptions import CommandFailed
from datetime import timedelta
from ocs_ci.ocs import constants
from ocs_ci.ocs.resources.pod import (
    get_pod_logs,
    get_ceph_tools_pod,
    get_mon_pod_id,
    wait_for_pods_to_be_running,
)

logger = logging.getLogger(__name__)


def check_for_read_pause(logreader_pods, start_time, end_time):
    """
    This checks for any read pause has occurred during the given
    window of start_time and end_time

    Args:
        logreader_pods (list): List of logreader pod objects
        start_time (datetime): datetime object representing the start time
        end_time (datetime): datetime object representing the end time

    Returns:
         Boolean : True if the pouase has occured else False

    """
    paused = False
    for pod in logreader_pods:
        pause_count = 0
        time_var = start_time
        pod_log = get_pod_logs(
            pod_name=pod.name, namespace=constants.STRETCH_CLUSTER_NAMESPACE
        )
        logger.info(f"Current pod: {pod.name}")
        while time_var <= (end_time + timedelta(minutes=1)):
            t_time = time_var.strftime("%H:%M")
            if f" {t_time}" not in pod_log:
                pause_count += 1
                logger.info(f"Read pause: {t_time}")
            else:
                logger.info(f"Read success: {t_time}")
            time_var = time_var + timedelta(minutes=1)
        if pause_count > 5:
            paused = True
            break
    return paused


def check_for_write_pause(logwriter_pod, log_files, start_time, end_time):
    """
    This checks for any read pause has occurred during the given
    window of start_time and end_time

    Args:
        logwriter_pod (Pod): Logwriter Pod object
        log_files (list): List representing the list of log files generated
        start_time (datetime): datetime object representing the start time
        end_time (datetime): datetime object representing the end time

    Returns:
         Boolean : True if the pouase has occured else False

    """
    paused = False
    for file_name in log_files:
        pause_count = 0
        file_log = logwriter_pod.exec_sh_cmd_on_pod(command=f"cat {file_name}")
        time_var = start_time
        logger.info(f"Current file: {file_name}")
        while time_var <= (end_time + timedelta(minutes=1)):
            t_time = time_var.strftime("%H:%M")
            if f"T{t_time}" not in file_log:
                pause_count += 1
                logger.info(f"Write pause: {t_time}")
            else:
                logger.info(f"Write success: {t_time}")
            time_var = time_var + timedelta(minutes=1)
        if pause_count > 5:
            paused = True
            break
    return paused


@retry(CommandFailed, tries=4, delay=5)
def get_logfile_map_from_logwriter_pods(logwriter_pods, is_rbd=False):
    """
    This function fetches all the logfiles generated by logwriter instances
    and maps it with a string representing the start time of the logging

    Args:
        logwriter_pods (List): List of logwriter pod objects
        is_rbd (bool): True if it's an RBD RWO workload else False
    Returns:
        Dict: Representing map containing file name key and start time value

    """
    log_file_map = {}

    if not is_rbd:
        for file_name in list(
            filter(
                None,
                (
                    logwriter_pods[0]
                    .exec_sh_cmd_on_pod(command="ls -l | awk 'NR>1' | awk '{print $9}'")
                    .split("\n")
                ),
            )
        ):
            start_time = (
                logwriter_pods[0]
                .exec_sh_cmd_on_pod(command=f"cat {file_name} | grep -i started")
                .split(" ")[0]
            )
            log_file_map[file_name] = start_time.split("T")[1]
    else:
        for logwriter_pod in logwriter_pods:
            log_file_map[logwriter_pod.name] = {}
            for file_name in logwriter_pod.exec_sh_cmd_on_pod(
                command="ls -l | awk 'NR>1' | awk '{print $9}'"
            ).split("\n"):
                if file_name not in ("", "lost+found"):
                    start_time = logwriter_pod.exec_sh_cmd_on_pod(
                        command=f"cat {file_name} | grep -i started"
                    ).split(" ")[0]
                    log_file_map[logwriter_pod.name][file_name] = start_time.split("T")[
                        1
                    ]

    return log_file_map


def fetch_connection_scores_for_mon(mon_pod):
    mon_pod_id = get_mon_pod_id(mon_pod)
    cmd = f"ceph daemon mon.{mon_pod_id} connection scores dump"
    return mon_pod.exec_cmd_on_pod(command=cmd, out_yaml_format=False)


def get_mon_quorum_ranks():
    ceph_tools_pod = get_ceph_tools_pod()
    out = dict(ceph_tools_pod.exec_cmd_on_pod(command="ceph quorum_status"))
    mon_quorum_ranks = {}
    for rank in list(out["quorum"]):
        mon_quorum_ranks[list(out["quorum_names"])[rank]] = rank
    return mon_quorum_ranks


def validate_conn_score(conn_score_map, quorum_ranks):
    for mon_id in quorum_ranks.keys():
        conn_score_str = conn_score_map[mon_id]
        conn_score = json.loads(conn_score_str)
        assert (
            conn_score["rank"] == quorum_ranks[mon_id]
        ), f"mon {mon_id} is not ranked {quorum_ranks[mon_id]}"
        pattern = r'"report":\s*{(?:[^}]+}\s*){4}(?:\s*}){2}'
        matches = re.findall(pattern, conn_score_str)
        validated = 0
        for j, match in enumerate(matches):
            report = json.loads("{" + str(match) + "}")
            current_rank = report["report"]["rank"]
            assert (
                current_rank == j
            ), f"Connection score is messed up \n {conn_score_str}"
            assert (
                int(current_rank) <= 4
            ), f"Connection score is messed up \n {conn_score_str}"
            if current_rank < 0:
                continue
            peer_pattern = r'"peer":\s*{[^}]+}'
            peer_matches = re.findall(peer_pattern, match)
            for i, peer in enumerate(peer_matches):
                peer = json.loads("{" + str(peer) + "}")
                assert (
                    current_rank != peer["peer"]["peer_rank"]
                ), f"Connection score is messed up! \n {conn_score_str}"
                if i >= current_rank:
                    i += 1
                assert (
                    i == peer["peer"]["peer_rank"]
                ), f"Connection score is messed up \n {conn_score_str}"
            validated += 1
        assert validated == 5, f"Connection score is messed up \n {conn_score_str}"
        logger.info("Connection score is valid")


@retry(CommandFailed, tries=4, delay=5)
def check_ceph_accessibility(timeout=30, delay=5, grace=15):
    command = (
        f"SECONDS=0;while true;do ceph -s;sleep {delay};duration=$SECONDS;"
        f"if [ $duration -ge {timeout} ];then break;fi;done"
    )
    ceph_tools_pod = get_ceph_tools_pod()
    if not wait_for_pods_to_be_running(pod_names=[ceph_tools_pod.name]):
        ceph_tools_pod.delete()
        logger.info(f"Deleted ceph tools pod {ceph_tools_pod.name}")
        time.sleep(5)
        ceph_tools_pod = get_ceph_tools_pod()
        wait_for_pods_to_be_running(pod_names=[ceph_tools_pod])
        logger.info(f"New ceph tools pod {ceph_tools_pod.name}")
    try:
        if (
            "monclient(hunting): authenticate timed out"
            in ceph_tools_pod.exec_sh_cmd_on_pod(
                command=command, timeout=timeout + grace
            )
        ):
            logger.warning("Ceph was hung for sometime.")
            return False
        return True
    except Exception as err:
        if "TimeoutExpired" in err.args[0]:
            logger.error("Ceph status check got timed out. maybe ceph is hung.")
            return False
        else:
            raise
