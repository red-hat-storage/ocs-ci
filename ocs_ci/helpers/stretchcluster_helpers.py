import logging
import json
import re
import time

from ocs_ci.utility.retry import retry
from ocs_ci.ocs.exceptions import CommandFailed, UnexpectedBehaviour
from datetime import timedelta
from ocs_ci.ocs import constants
from ocs_ci.ocs.resources.pod import (
    get_pod_logs,
    get_ceph_tools_pod,
    get_mon_pod_id,
    wait_for_pods_to_be_running,
    Pod,
    get_pods_having_label,
)

logger = logging.getLogger(__name__)


@retry(CommandFailed, tries=10, delay=10)
def check_for_read_pause(logreader_pods, start_time, end_time):
    """
    This checks for any read pause has occurred during the given
    window of start_time and end_time

    Args:
        logreader_pods (list): List of logreader pod objects
        start_time (datetime): datetime object representing the start time
        end_time (datetime): datetime object representing the end time

    Returns:
         Boolean : True if the pouase has occured else False

    """
    paused = 0
    for pod in logreader_pods:
        pause_count = 0
        time_var = start_time
        pod_log = get_pod_logs(
            pod_name=pod.name, namespace=constants.STRETCH_CLUSTER_NAMESPACE
        )
        logger.info(f"Current pod: {pod.name}")
        while time_var <= (end_time + timedelta(minutes=1)):
            t_time = time_var.strftime("%H:%M")
            if f" {t_time}" not in pod_log:
                pause_count += 1
                logger.info(f"Read pause: {t_time}")
            else:
                logger.info(f"Read success: {t_time}")
            time_var = time_var + timedelta(minutes=1)
        if pause_count > 5:
            paused += 1
    return paused


@retry(CommandFailed, tries=10, delay=10)
def check_for_write_pause(logwriter_pod, log_files, start_time, end_time):
    """
    This checks for any read pause has occurred during the given
    window of start_time and end_time

    Args:
        logwriter_pod (Pod): Logwriter Pod object
        log_files (list): List representing the list of log files generated
        start_time (datetime): datetime object representing the start time
        end_time (datetime): datetime object representing the end time

    Returns:
         Boolean : True if the pouase has occured else False

    """
    paused = 0
    for file_name in log_files:
        pause_count = 0
        file_log = logwriter_pod.exec_sh_cmd_on_pod(command=f"cat {file_name}")
        time_var = start_time
        logger.info(f"Current file: {file_name}")
        while time_var <= (end_time + timedelta(minutes=1)):
            t_time = time_var.strftime("%H:%M")
            if f"T{t_time}" not in file_log:
                pause_count += 1
                logger.info(f"Write pause: {t_time}")
            else:
                logger.info(f"Write success: {t_time}")
            time_var = time_var + timedelta(minutes=1)
        if pause_count > 5:
            paused += 1
    return paused


@retry(CommandFailed, tries=10, delay=5)
def get_logfile_map_from_logwriter_pods(logwriter_pods, is_rbd=False):
    """
    This function fetches all the logfiles generated by logwriter instances
    and maps it with a string representing the start time of the logging

    Args:
        logwriter_pods (List): List of logwriter pod objects
        is_rbd (bool): True if it's an RBD RWO workload else False
    Returns:
        Dict: Representing map containing file name key and start time value

    """
    log_file_map = {}

    if not is_rbd:
        for file_name in list(
            filter(
                None,
                (
                    logwriter_pods[0]
                    .exec_sh_cmd_on_pod(command="ls -l | awk 'NR>1' | awk '{print $9}'")
                    .split("\n")
                ),
            )
        ):
            start_time = (
                logwriter_pods[0]
                .exec_sh_cmd_on_pod(command=f"cat {file_name} | grep -i started")
                .split(" ")[0]
            )
            log_file_map[file_name] = start_time.split("T")[1]
    else:
        for logwriter_pod in logwriter_pods:
            log_file_map[logwriter_pod.name] = {}
            for file_name in logwriter_pod.exec_sh_cmd_on_pod(
                command="ls -l | awk 'NR>1' | awk '{print $9}'"
            ).split("\n"):
                if file_name not in ("", "lost+found"):
                    start_time = logwriter_pod.exec_sh_cmd_on_pod(
                        command=f"cat {file_name} | grep -i started"
                    ).split(" ")[0]
                    log_file_map[logwriter_pod.name][file_name] = start_time.split("T")[
                        1
                    ]

    return log_file_map


def fetch_connection_scores_for_mon(mon_pod):
    mon_pod_id = get_mon_pod_id(mon_pod)
    cmd = f"ceph daemon mon.{mon_pod_id} connection scores dump"
    return mon_pod.exec_cmd_on_pod(command=cmd, out_yaml_format=False)


def get_mon_quorum_ranks():
    ceph_tools_pod = get_ceph_tools_pod()
    out = dict(ceph_tools_pod.exec_cmd_on_pod(command="ceph quorum_status"))
    mon_quorum_ranks = {}
    for rank in list(out["quorum"]):
        mon_quorum_ranks[list(out["quorum_names"])[rank]] = rank
    return mon_quorum_ranks


def validate_conn_score(conn_score_map, quorum_ranks):
    for mon_id in quorum_ranks.keys():
        conn_score_str = conn_score_map[mon_id]
        conn_score = json.loads(conn_score_str)
        assert (
            conn_score["rank"] == quorum_ranks[mon_id]
        ), f"mon {mon_id} is not ranked {quorum_ranks[mon_id]}"
        pattern = r'"report":\s*{(?:[^}]+}\s*){4}(?:\s*}){2}'
        matches = re.findall(pattern, conn_score_str)
        validated = 0
        for j, match in enumerate(matches):
            report = json.loads("{" + str(match) + "}")
            current_rank = report["report"]["rank"]
            assert (
                current_rank == j
            ), f"Connection score is messed up \n {conn_score_str}"
            assert (
                int(current_rank) <= 4
            ), f"Connection score is messed up \n {conn_score_str}"
            if current_rank < 0:
                continue
            peer_pattern = r'"peer":\s*{[^}]+}'
            peer_matches = re.findall(peer_pattern, match)
            for i, peer in enumerate(peer_matches):
                peer = json.loads("{" + str(peer) + "}")
                assert (
                    current_rank != peer["peer"]["peer_rank"]
                ), f"Connection score is messed up! \n {conn_score_str}"
                if i >= current_rank:
                    i += 1
                assert (
                    i == peer["peer"]["peer_rank"]
                ), f"Connection score is messed up \n {conn_score_str}"
            validated += 1
        assert validated == 5, f"Connection score is messed up \n {conn_score_str}"
        logger.info("Connection score is valid")


@retry(CommandFailed, tries=15, delay=5)
def check_ceph_accessibility(timeout=30, delay=5, grace=15):
    command = (
        f"SECONDS=0;while true;do ceph -s;sleep {delay};duration=$SECONDS;"
        f"if [ $duration -ge {timeout} ];then break;fi;done"
    )
    ceph_tools_pod = get_ceph_tools_pod()
    if not wait_for_pods_to_be_running(pod_names=[ceph_tools_pod.name]):
        ceph_tools_pod.delete()
        logger.info(f"Deleted ceph tools pod {ceph_tools_pod.name}")
        time.sleep(5)
        ceph_tools_pod = get_ceph_tools_pod()
        wait_for_pods_to_be_running(pod_names=[ceph_tools_pod])
        logger.info(f"New ceph tools pod {ceph_tools_pod.name}")
    try:
        if (
            "monclient(hunting): authenticate timed out"
            in ceph_tools_pod.exec_sh_cmd_on_pod(
                command=command, timeout=timeout + grace
            )
        ):
            logger.warning("Ceph was hung for sometime.")
            return False
        return True
    except Exception as err:
        if "TimeoutExpired" in err.args[0]:
            logger.error("Ceph status check got timed out. maybe ceph is hung.")
            return False
        else:
            raise


@retry(CommandFailed, tries=10, delay=10)
def check_for_data_corruption(
    logreader_pods=None, logwriter_rbd_pods=None, rbd_logfile_map=None
):
    if logreader_pods:
        for pod_name in logreader_pods:
            pod_logs = get_pod_logs(
                pod_name=pod_name, namespace=constants.STRETCH_CLUSTER_NAMESPACE
            )
            return "corrupt" not in pod_logs

    if logwriter_rbd_pods and rbd_logfile_map:
        for logwriter_pod in logwriter_rbd_pods:
            output = logwriter_pod.exec_cmd_on_pod(
                command=f"/opt/logreader.py -t 5 {list(rbd_logfile_map[logwriter_pod.name].keys())[0]} -d",
                out_yaml_format=False,
            )
            return "corrupt" not in output
    else:
        assert (
            False
        ), "Couldn't check for data corruption in RBD workloads, some arguments are None maybe"
    return True


@retry(UnexpectedBehaviour, tries=20, delay=10)
def get_logwriter_reader_pods(
    logwriter_pods_tup=(None, None, 4),
    logreader_pods_tup=(None, None, 4),
    namespace=constants.STRETCH_CLUSTER_NAMESPACE,
):
    """
    This function helps to fetch the pod info of logwriter and logreader workloads

    Args:
        logwriter_pods_tup (tuple): (True if want to fetch Logwriter pods info,
                                    labels of Logwriter pods app,
                                    expected number of replicas)
        logreader_pods_tup (tuple): (True if want to fetch Logreader pods info,
                                    labels of Logreader pods app,
                                    expected number of replicas)
        namespace (str): namespace

    Returns:
        List, List: Lists containing logwriter pods and logreader pods info

    Raises:
        UnexpectedBehaviour: If expected replicas of pods are not found UnexpectedBehaviour excpetion is thrown
    """
    logwriter_pods = []
    logreader_pods = []
    if logwriter_pods_tup[0] is not None:
        logwriter_pods = [
            Pod(**pod)
            for pod in get_pods_having_label(
                label=logwriter_pods_tup[1],
                namespace=constants.STRETCH_CLUSTER_NAMESPACE,
                statuses=["Running", "Creating", "Pending"],
            )
        ]
        logger.info(f"Logwriter: {[pod.name for pod in logwriter_pods]}")
        if len(logwriter_pods) != logwriter_pods_tup[2]:
            logger.warning(
                "Seems like some of the logwriter pods are not stabilized yet"
            )
            raise UnexpectedBehaviour

    if logreader_pods_tup[0] is not None:
        logreader_pods = [
            Pod(**pod)
            for pod in get_pods_having_label(
                label=logreader_pods_tup[1],
                namespace=constants.STRETCH_CLUSTER_NAMESPACE,
                statuses=["Running", "Succeeded", "Creating", "Pending"],
            )
        ]
        logger.info(f"Logreader: {[pod.name for pod in logreader_pods]}")
        if len(logreader_pods) != logreader_pods_tup[2]:
            logger.warning(
                "Seems like some of the logreader pods are not stabilized yet"
            )
            raise UnexpectedBehaviour

    return logwriter_pods, logreader_pods
